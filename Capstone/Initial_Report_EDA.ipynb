{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34168066",
   "metadata": {},
   "source": [
    "E-COMMERCE CUSTOMER PURCHASE PREDICTION\n",
    "\n",
    "**Problem Statement** \n",
    "Can we predict customer purchase using browsing patterns, session characteristics, and product features?\n",
    "\n",
    "Data source \n",
    "https://data.rees46.com/datasets/marketplace/2019-Nov.csv.gz \n",
    "\n",
    "About the dataset\n",
    "* Dataset contains user browsing and purchasing activities from a multi-category online store. Includes over 67.5 million events (views, cart additions, and purchases) across various product categories.\n",
    "\n",
    "Please read the [Readme.md](Readme.md) file for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8932b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load full dataset for analysis\n",
    "data_url = \"https://data.rees46.com/datasets/marketplace/2019-Nov.csv.gz\"\n",
    "df = pd.read_csv(data_url, compression='gzip')\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6188d6f",
   "metadata": {},
   "source": [
    "#### Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ee8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame(\n",
    "    {\n",
    "        'Column': missing_data.index,\n",
    "        'Missing Count': missing_data.values,\n",
    "        'Missing Percentage': missing_percent.values\n",
    "    }\n",
    ")\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"Missing Data Summary:\")\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"No missing data found!\")\n",
    "\n",
    "# Data quality checks\n",
    "print(f\"Total events: {len(df):,}\")\n",
    "print(f\"Unique users: {df['user_id'].nunique():,}\")\n",
    "print(f\"Unique products: {df['product_id'].nunique():,}\")\n",
    "print(f\"Date range: {df['event_time'].min()} to {df['event_time'].max()}\")\n",
    "print(f\"Zero prices: {(df['price'] == 0).sum():,} ({(df['price'] == 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Event type distribution\n",
    "event_dist = df['event_type'].value_counts()\n",
    "event_pct = (event_dist / len(df) * 100).round(2)\n",
    "for event, count in event_dist.items():\n",
    "    print(f\"{event}: {count:,} ({event_pct[event]}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2606c",
   "metadata": {},
   "source": [
    "#### Target Variable Creation\n",
    "We will be creating user-level purchase behavior for classification. \n",
    "And we will be creating a target variable that will be 1 if the user has made a purchase and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fdb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_by_type(event_series, type):\n",
    "    return (event_series == type).sum()\n",
    "\n",
    "user_features = df.groupby('user_id').agg(\n",
    "    total_events=('event_type', 'count'),\n",
    "    views=('event_type', lambda x: count_by_type(x, 'view')),\n",
    "    unique_products=('product_id', 'nunique'),\n",
    "    unique_categories=('category_id', 'nunique'),\n",
    "    avg_price=('price', 'mean'),\n",
    "    max_price=('price', 'max'),\n",
    "    total_price=('price', 'sum'),\n",
    "    sessions=('user_session', 'nunique'),\n",
    "    first_event=('event_time', 'min'),\n",
    "    last_event=('event_time', 'max')\n",
    ").round(2)\n",
    "\n",
    "# Create binary target: 1 if user made any purchase, 0 otherwise\n",
    "purchase_users = set(df[df['event_type'] == 'purchase']['user_id'].unique())\n",
    "user_features['made_purchase'] = user_features.index.isin(purchase_users).astype(int)\n",
    "\n",
    "print(f\"Total # of users: {len(user_features)}\")\n",
    "target_dist = user_features['made_purchase'].value_counts()\n",
    "print(f\"No Purchase (0): {target_dist[0]:,} ({target_dist[0]/len(user_features)*100:.1f}%)\")\n",
    "print(f\"Made Purchase (1): {target_dist[1]:,} ({target_dist[1]/len(user_features)*100:.1f}%)\")\n",
    "print(f\"Conversion Rate: {user_features['made_purchase'].mean()*100:.2f}%\")\n",
    "\n",
    "user_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9793647",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "In this step, we will be adding features to the dataset for, \n",
    "1. Session-level features\n",
    "2. User interaction features\n",
    "3. Product-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ccada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "\n",
    "# 1. Session features\n",
    "session_features = df.groupby('user_id').agg({\n",
    "    'event_time': lambda x: (x.max() - x.min()).total_seconds() / 60\n",
    "}).rename(columns={'event_time': 'session_duration_minutes'})\n",
    "\n",
    "# 2. user interaction pattern features\n",
    "user_features['events_per_session'] = user_features['total_events'] / user_features['sessions']\n",
    "\n",
    "# 3. Product exploration behavior\n",
    "user_features['category_exploration'] = user_features['unique_categories'] / user_features['unique_products']\n",
    "user_features['product_focus'] = user_features['total_events'] / user_features['unique_products']\n",
    "user_features['price_range'] = user_features['max_price'] - user_features['avg_price']\n",
    "\n",
    "user_features = user_features.join(session_features)\n",
    "\n",
    "print(f\"Total features: {len(user_features.columns) - 1} (excluding target)\")\n",
    "\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "display(user_features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d567c5",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "Here we will be analyzing the dataset to understand the data better. \n",
    "There are 4 plots that are created below, \n",
    "1. Conversion by number of views\n",
    "2. Conversion by average price viewed\n",
    "3. Conversion by session duration\n",
    "4. Conversion by product explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conversion_by_feature(data, feature_col, bins, labels, title, ax, color='skyblue'):\n",
    "    groups = pd.cut(data[feature_col], bins=bins, labels=labels)\n",
    "    conversion_by_feature = data.groupby(groups)['made_purchase'].mean()\n",
    "    \n",
    "    conversion_by_feature.plot(kind='bar', ax=ax, title=title, color=color)\n",
    "    ax.set_ylabel('Conversion Rate')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    return conversion_by_feature\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Conversion by number of views\n",
    "conversion_views = plot_conversion_by_feature(\n",
    "    data=user_features,\n",
    "    feature_col='views',\n",
    "    bins=[0, 1, 5, 10, 20, 100],\n",
    "    labels=['1', '2-5', '6-10', '11-20', '20+'],\n",
    "    title='Conversion Rate by Number of Views',\n",
    "    ax=axes[0,0],\n",
    "    color='skyblue'\n",
    ")\n",
    "\n",
    "# Plot 2: Conversion by average price\n",
    "conversion_price = plot_conversion_by_feature(\n",
    "    data=user_features,\n",
    "    feature_col='avg_price',\n",
    "    bins=5,\n",
    "    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'],\n",
    "    title='Conversion Rate by Average Price Viewed',\n",
    "    ax=axes[0,1],\n",
    "    color='skyblue'\n",
    ")\n",
    "\n",
    "# Plot 3: Conversion by session duration\n",
    "conversion_duration = plot_conversion_by_feature(\n",
    "    data=user_features,\n",
    "    feature_col='session_duration_minutes',\n",
    "    bins=[0, 1, 5, 15, 60, 1000],\n",
    "    labels=['<1min', '1-5min', '5-15min', '15-60min', '60min+'],\n",
    "    title='Conversion Rate by Session Duration',\n",
    "    ax=axes[1,0],\n",
    "    color='skyblue'\n",
    ")\n",
    "\n",
    "# Plot 4: Conversion by unique products viewed\n",
    "conversion_exploration = plot_conversion_by_feature(\n",
    "    data=user_features,\n",
    "    feature_col='unique_products',\n",
    "    bins=[0, 1, 3, 5, 10, 100],\n",
    "    labels=['1', '2-3', '4-5', '6-10', '10+'],\n",
    "    title='Conversion Rate by Products Explored',\n",
    "    ax=axes[1,1],\n",
    "    color='skyblue'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Overall conversion rate: {user_features['made_purchase'].mean()*100:.2f}%\")\n",
    "print(f\"Highest converting view group: {conversion_views.idxmax()}\")\n",
    "print(f\"Highest converting price group: {conversion_price.idxmax()}\")\n",
    "print(f\"Optimal session duration: {conversion_duration.idxmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0584a86e",
   "metadata": {},
   "source": [
    "#### Category & Brand Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3153fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_events = df[df['event_type'] == 'purchase'].copy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Top categories by purchases\n",
    "top_categories = purchase_events['category_code'].value_counts().head(10)\n",
    "top_categories.plot(kind='barh', ax=axes[0,0], title='Top 10 Categories by Purchases')\n",
    "axes[0,0].set_xlabel('Number of Purchases')\n",
    "\n",
    "# Top brands by purchases\n",
    "top_brands = purchase_events['brand'].value_counts().head(10)\n",
    "top_brands.plot(kind='barh', ax=axes[0,1], title='Top 10 Brands by Purchases')\n",
    "axes[0,1].set_xlabel('Number of Purchases')\n",
    "\n",
    "# Price distribution of purchases\n",
    "axes[1,0].hist(purchase_events['price'], bins=30, alpha=0.7, color='skyblue')\n",
    "axes[1,0].set_title('Distribution of Purchase Prices')\n",
    "axes[1,0].set_xlabel('Price')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# Purchase timing\n",
    "purchase_events['hour'] = purchase_events['event_time'].dt.hour\n",
    "hourly_purchases = purchase_events['hour'].value_counts().sort_index()\n",
    "hourly_purchases.plot(kind='line', ax=axes[1,1], title='Purchases by Hour of Day', marker='o')\n",
    "axes[1,1].set_xlabel('Hour')\n",
    "axes[1,1].set_ylabel('Number of Purchases')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total purchases: {len(purchase_events):,}\")\n",
    "print(f\"Average purchase price: ${purchase_events['price'].mean():.2f}\")\n",
    "print(f\"Most popular category: {top_categories.index[0]}\")\n",
    "print(f\"Most popular brand: {top_brands.index[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9492cfce",
   "metadata": {},
   "source": [
    "#### Correlation Analysis & Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_corr = user_features.select_dtypes(include=[np.number]).fillna(0)\n",
    "\n",
    "corr_matrix = features_for_corr.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    center=0, \n",
    "    square=True,\n",
    "    mask=mask,\n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "target_corr_clean = corr_matrix['made_purchase'].abs().sort_values(ascending=False)\n",
    "print(\"Features most correlated with purchase behavior:\")\n",
    "for feature, corr in target_corr_clean.drop('made_purchase').head(10).items():\n",
    "    print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "important_clean_features = target_corr_clean.drop('made_purchase').head(10).index.tolist()\n",
    "print(f\"Top 10 clean features for modeling: {important_clean_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c637a2",
   "metadata": {},
   "source": [
    "#### Baseline Modeling\n",
    "Below we will use Logistic Regression and Random Forest to create a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236116a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, precision_score,\\\n",
    "    accuracy_score, recall_score, f1_score\n",
    "\n",
    "X = user_features[important_clean_features]\n",
    "y = user_features['made_purchase']\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples/Users: {len(X)}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(X_train)} samples\")\n",
    "print(f\"Testing: {len(X_test)} samples\")\n",
    "\n",
    "# Pipeline 1: Logistic Regression (needs scaling)\n",
    "lr_pipeline = Pipeline(\n",
    "    [\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pipeline 2: Random Forest\n",
    "rf_pipeline = Pipeline(\n",
    "    [\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100))\n",
    "    ]\n",
    ")\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': lr_pipeline,\n",
    "    'Random Forest': rf_pipeline\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1_Score': f1_score(y_test, y_pred),\n",
    "        'AUC_ROC': roc_auc_score(y_test, y_prob)\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"Running {name}...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    results[name] = evaluate_model(model=pipeline, \n",
    "                                   X_test=X_test,\n",
    "                                   y_test=y_test)\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "display(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
